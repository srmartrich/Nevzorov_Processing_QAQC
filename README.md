# Nevzorov_Processing_QAQC general overview README
This repository consists of Processing and QAQC codes for the Nevzorov Hot Wire Probe. This will have multiple branches for processing and QAQC of the traditional cloud physics observations at 1-Hz sampling frequency, and a High Rate sampling frequency version (25-Hz was used during development). The development of this code was for the Cold Air-Outbreak Experiment in the Sub-Arctic Region (CAESAR) field campaign funded by the National Science Foundation (NSF). These codes, as they are now, is for both the National Center for Atmospheric Research Earth Observing Laboratory (NCAR-EOL), and, its main purpose, the University of Wyoming King Air (NSF-UWKA) facility. NCAR-EOL recently acquired a Nevzorov and likely will utilize the code within this repository, and the processing steps and techniques illustrated within my Masters Thesis.

## Processing overview
These codes utilize an independent two dimensional approach, where it is the convective heat term $K$ independently against two seperate variables. This theoretically can be done in a three dimensional approach, where the best fit lines for $K$ is not independent from each dependence (Airspeed and Density), but rather one best fit plane that characterizes $K$ given some Airspeed and Density data. To understand the three dimensional approach requires significant time to characterize the effectiveness of this method, that is beyond the scope of this work. Additionally, a machine learning approach may prove more beneficial to encompass this three dimensionality without having to do much in terms of by-hand statistics. 

Density in this case is colloquial verbage for air density, and is not represented in the code. The reasons will be outlined in the thesis, however, to process the data pressure was used in place of density given the ideal gas law relationship. 
##### ** It is recommended to start by finding the dependence for indicated airspeed ($U_{ind}$) first, then the dependence for static pressure ($P_{stat}$) ** 
This is because during the speed runs, there generally are more variations in speed than there are in pressure levels for each speed runs. This may not always be true, in which either could be done first. (In theory, either fit could be done first, but $K$ v.s. $U_{ind}$ is more conceptually intuitive when applying it to the next fit than the oppposite order)

## QAQC overview
QAQC is significantly more time consuming than the Processing of the data, the QAQC involves looking at the derived baseline given how the data was processed and isolating if any base line drifts are evident. If so, the steps for QAQC can be done. This code for the QAQC specifically, may not be overly useful as everyone has their own preferences in plotting and associated coding logics. This code is here for a guide in how one can do it, than it is for one to be able to use it.

The idea for QAQC when isolating baseline drifts is to follow these steps:
* This is where we compare our processed Nevzorov Hot-wire Probe data to the other instruments available on whatever aircraft it was on. For CAESAR, the raw processed data was compared against the Cloud Droplet Probe (CDP) and KING Hot-wire Probe for initial QAQC. Other instruments were also compared to the Nevzorov data much later (after initial QAQC baseline adjustments) such as the Holographic Detector of Clouds (HOLODEC-II), two-dimensional stereo cloud probe (2D-S) and the Counter-flow Virtual Impactor inlet (CVI) through NCAR-EOL. These were to ensure that the Nevzorov compares well against all cloud probes and not a small subset, more explicit reasons are outlined in the thesis.
* A baseline for LWC and TWC is in need of adjustment if the baseline should be zero, and is significantly not zero. "Significantly" is colloquially defined as outside of the probes internal uncertainty, such that the LWC sensitivity is ± 0.01 - 0.02 $(g /m^{3})$ and the TWC sensitivity is ± 0.02 - 0.03 $(g /m^{3})$. Thus, the baselines outside of these ranges can be corrected to them respectively. In this code, 0.01. 0.02 were used as good data proxies, where baselines beyond ± 0.01 were corrected to be within ± 0.01 if applicable, and denoted as excellent quality as the original baseline (of 0.012 for example) was within the LWC uncertainty.
* It is important to compare each data point across flights to the comparison instruments (i.e., CDP and KING) in a scatter plot with a 20% uncertainty curtain to ensure the majority of data compared is within ± 20% from the 1 to 1 line.
* At this point now:
* Find a noticible baseline drift out of cloud, where the baseline should be at zero. The thesis explains in-depth why baseline drifts occur, their causes, and their mitigation in processing.
* Then once a potential drift is isolated, it is important to only change baselines that are 10 seconds or longer in baselines to be changed.
* If the baseline can be changed to be within the instrument uncertainty (LWC: 0.01 for excellent, 0.02 for good) then correct it to each region.
* Adjust the baseline consistently across a 10s of second plus segment for consistency. This adjustment can extend into cloud regions if a noticable drift occurs in cloud, otherwise if one such drift can not be identified in cloud, it would have to start when the baseline should return to zero.

## These are general overviews, and additional readme's will be in each branch associated with each code.
